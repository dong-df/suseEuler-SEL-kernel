From 326d641b8338bbaec5623e091a5249492d8c43ff Mon Sep 17 00:00:00 2001
From: Yu Kuai <yukuai3@huawei.com>
Date: Tue, 8 Mar 2022 22:06:14 +0800
Subject: [PATCH] blk-mq: decrease pending_queues when it expires
Patch-mainline: Not yet, from openEuler
References: bsn#22
openEuler-commit: 326d641b8338bbaec5623e091a5249492d8c43ff
Modified-by-SEL: Yes, refreshed due to different context


hulk inclusion
category: performance
bugzilla: https://gitee.com/openeuler/kernel/issues/I4S8DW

---------------------------

If pending_queues is increased once, it will only be decreased when
nr_active is zero, and that will lead to the under-utilization of
host tags because pending_queues is non-zero and the available
tags for the queue will be max(host tags / active_queues, 4)
instead of the needed tags of the queue.

Fix it by adding an expiration time for the increasement of pending_queues,
and decrease it when it expires, so pending_queues will be decreased
to zero if there is no tag allocation failure, and the available tags
for the queue will be the whole host tags.

Signed-off-by: Yu Kuai <yukuai3@huawei.com>
Reviewed-by: Hou Tao <houtao1@huawei.com>
Signed-off-by: Zheng Zengkai <zhengzengkai@huawei.com>
Signed-off-by: Guoqing Jiang <guoqing.jiang@suse.com>
---
 block/blk-mq-debugfs.c |   15 +++++++++++++++
 block/blk-mq-tag.c     |   34 +++++++++++++++++++++++++++++-----
 block/blk-mq-tag.h     |    6 +++---
 block/blk-mq.c         |   10 ++++++----
 include/linux/blk-mq.h |    6 ++++++
 include/linux/blkdev.h |    1 +
 6 files changed, 60 insertions(+), 12 deletions(-)

--- a/block/blk-mq-debugfs.c
+++ b/block/blk-mq-debugfs.c
@@ -623,6 +623,20 @@ static int hctx_dispatch_busy_show(void
 	return 0;
 }
 
+static int hctx_dtag_wait_time_show(void *data, struct seq_file *m)
+{
+	struct blk_mq_hw_ctx *hctx = data;
+	unsigned int time = 0;
+
+	if (test_bit(BLK_MQ_S_DTAG_WAIT, &hctx->state))
+		time = jiffies_to_msecs(jiffies -
+					READ_ONCE(hctx->dtag_wait_time));
+
+	seq_printf(m, "%u\n", time);
+
+	return 0;
+}
+
 #define CTX_RQ_SEQ_OPS(name, type)					\
 static void *ctx_##name##_rq_list_start(struct seq_file *m, loff_t *pos) \
 	__acquires(&ctx->lock)						\
@@ -793,6 +807,7 @@ static const struct blk_mq_debugfs_attr
 	{"active", 0400, hctx_active_show},
 	{"dispatch_busy", 0400, hctx_dispatch_busy_show},
 	{"type", 0400, hctx_type_show},
+	{"dtag_wait_time_ms", 0400, hctx_dtag_wait_time_show},
 	{},
 };
 
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -16,6 +16,8 @@
 #include "blk-mq-sched.h"
 #include "blk-mq-tag.h"
 
+#define BLK_MQ_DTAG_WAIT_EXPIRE (5 * HZ)
+
 /*
  * If a previously inactive queue goes active, bump the active user count.
  * We need to do this before try to allocate driver tag, then even if fail
@@ -77,32 +79,52 @@ void __blk_mq_dtag_busy(struct blk_mq_hw
 {
 	if (blk_mq_is_sbitmap_shared(hctx->flags)) {
 		struct request_queue *q = hctx->queue;
-		struct blk_mq_tag_set *set = q->tag_set;
 
 		if (!test_bit(QUEUE_FLAG_HCTX_WAIT, &q->queue_flags) &&
-		    !test_and_set_bit(QUEUE_FLAG_HCTX_WAIT, &q->queue_flags))
+		    !test_and_set_bit(QUEUE_FLAG_HCTX_WAIT, &q->queue_flags)) {
+			WRITE_ONCE(hctx->dtag_wait_time, jiffies);
 			atomic_inc(&hctx->tags->pending_queues);
+		}
 	} else {
 		if (!test_bit(BLK_MQ_S_DTAG_WAIT, &hctx->state) &&
-		    !test_and_set_bit(BLK_MQ_S_DTAG_WAIT, &hctx->state))
+		    !test_and_set_bit(BLK_MQ_S_DTAG_WAIT, &hctx->state)) {
+			WRITE_ONCE(hctx->dtag_wait_time, jiffies);
 			atomic_inc(&hctx->tags->pending_queues);
+		}
 	}
 }
 
-void __blk_mq_dtag_idle(struct blk_mq_hw_ctx *hctx)
+void __blk_mq_dtag_idle(struct blk_mq_hw_ctx *hctx, bool force)
 {
 	struct blk_mq_tags *tags = hctx->tags;
 	struct request_queue *q = hctx->queue;
-	struct blk_mq_tag_set *set = q->tag_set;
 
 	if (blk_mq_is_sbitmap_shared(hctx->flags)) {
+		if (!test_bit(QUEUE_FLAG_HCTX_WAIT, &q->queue_flags))
+			return;
+
+		if (!force && time_before(jiffies,
+					  READ_ONCE(hctx->dtag_wait_time) +
+					  BLK_MQ_DTAG_WAIT_EXPIRE))
+			return;
+
 		if (!test_and_clear_bit(QUEUE_FLAG_HCTX_WAIT,
 					&q->queue_flags))
 			return;
+		WRITE_ONCE(hctx->dtag_wait_time, jiffies);
 		atomic_dec(&tags->pending_queues);
 	} else {
+		if (!test_bit(BLK_MQ_S_DTAG_WAIT, &hctx->state))
+			return;
+
+		if (!force && time_before(jiffies,
+					  READ_ONCE(hctx->dtag_wait_time) +
+					  BLK_MQ_DTAG_WAIT_EXPIRE))
+			return;
+
 		if (!test_and_clear_bit(BLK_MQ_S_DTAG_WAIT, &hctx->state))
 			return;
+		WRITE_ONCE(hctx->dtag_wait_time, jiffies);
 		atomic_dec(&tags->pending_queues);
 	}
 }
@@ -206,6 +228,8 @@ unsigned int blk_mq_get_tag(struct blk_m
 	sbitmap_finish_wait(bt, ws, &wait);
 
 found_tag:
+	if (!data->q->elevator)
+		blk_mq_dtag_idle(data->hctx, false);
 	/*
 	 * Give up this allocation if the hctx is inactive.  The caller will
 	 * retry on an active hctx.
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -81,7 +81,7 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx);
 extern void __blk_mq_dtag_busy(struct blk_mq_hw_ctx *hctx);
-extern void __blk_mq_dtag_idle(struct blk_mq_hw_ctx *hctx);
+extern void __blk_mq_dtag_idle(struct blk_mq_hw_ctx *hctx, bool force);
 
 
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
@@ -108,12 +108,12 @@ static inline void blk_mq_dtag_busy(stru
 	__blk_mq_dtag_busy(hctx);
 }
 
-static inline void blk_mq_dtag_idle(struct blk_mq_hw_ctx *hctx)
+static inline void blk_mq_dtag_idle(struct blk_mq_hw_ctx *hctx, bool force)
 {
 	if (!(mq_unfair_dtag && (hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)))
 		return;
 
-	__blk_mq_dtag_idle(hctx);
+	__blk_mq_dtag_idle(hctx, force);
 }
 
 static inline bool blk_mq_tag_is_reserved(struct blk_mq_tags *tags,
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -556,7 +556,7 @@ void blk_mq_free_request(struct request
 		__blk_mq_dec_active_requests(hctx);
 		if (mq_unfair_dtag && !__blk_mq_active_requests(hctx)) {
 			blk_mq_tag_idle(hctx);
-			blk_mq_dtag_idle(hctx);
+			blk_mq_dtag_idle(hctx, true);
 		}
 	}
 
@@ -1027,7 +1027,7 @@ static void blk_mq_timeout_work(struct w
 			/* the hctx may be unmapped, so check it here */
 			if (blk_mq_hw_queue_mapped(hctx)) {
 				blk_mq_tag_idle(hctx);
-				blk_mq_dtag_idle(hctx);
+				blk_mq_dtag_idle(hctx, true);
 			}
 		}
 	}
@@ -1138,6 +1138,7 @@ static bool __blk_mq_get_driver_tag(stru
 		return false;
 	}
 
+	blk_mq_dtag_idle(rq->mq_hctx, false);
 	rq->tag = tag + tag_offset;
 	return true;
 }
@@ -2762,7 +2763,7 @@ static void blk_mq_exit_hctx(struct requ
 
 	if (blk_mq_hw_queue_mapped(hctx)) {
 		blk_mq_tag_idle(hctx);
-		blk_mq_dtag_idle(hctx);
+		blk_mq_dtag_idle(hctx, true);
 	}
 
 	if (blk_queue_init_done(q))
@@ -2863,6 +2864,7 @@ blk_mq_alloc_hctx(struct request_queue *
 	INIT_LIST_HEAD(&hctx->dispatch);
 	hctx->queue = q;
 	hctx->flags = set->flags & ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	hctx->dtag_wait_time = jiffies;
 
 	INIT_LIST_HEAD(&hctx->hctx_list);
 
@@ -3106,7 +3108,7 @@ static void queue_set_hctx_shared(struct
 			hctx->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
 		} else {
 			blk_mq_tag_idle(hctx);
-			blk_mq_dtag_idle(hctx);
+			blk_mq_dtag_idle(hctx, true);
 			hctx->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
 		}
 	}
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -172,6 +172,12 @@ struct blk_mq_hw_ctx {
 	 */
 	struct list_head	hctx_list;
 
+	/**
+	 * @dtag_wait_time: record when hardware queue is pending, specifically
+	 * when BLK_MQ_S_DTAG_WAIT is set in state.
+	 */
+	unsigned long		dtag_wait_time;
+
 	KABI_RESERVE(1)
 	KABI_RESERVE(2)
 	KABI_RESERVE(3)
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -602,6 +602,7 @@ struct request_queue {
 
 	size_t			cmd_size;
 
+	unsigned long		dtag_wait_time;
 	KABI_RESERVE(1)
 	KABI_RESERVE(2)
 	KABI_RESERVE(3)
