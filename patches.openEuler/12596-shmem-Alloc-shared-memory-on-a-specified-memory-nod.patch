From 48554873145a2633952353faa527e7510e853dba Mon Sep 17 00:00:00 2001
From: Wang Wensheng <wangwensheng4@huawei.com>
Date: Thu, 30 Dec 2021 16:26:54 +0800
Subject: [PATCH] shmem: Alloc shared memory on a specified memory node
Patch-mainline: Not yet, from openEuler
References: bsn#22
openEuler-commit: 48554873145a2633952353faa527e7510e853dba
Modified-by-SEL: Yes, refreshed due to context change


ascend inclusion
category: Feature
bugzilla: https://gitee.com/openeuler/kernel/issues/I4NDAW
CVE: NA

-------------------

In some scenarios, there are memory only numa nodes used for designed
process groups. Users need to specify memory nodes to apply for shared
memory.

Here we extend shmem_alloc_and_acct_page() to accept an extra node_id
and supply that node_id through share_pool interface in later patches.

Signed-off-by: Peng Wu <wupeng58@huawei.com>
Signed-off-by: Wang Wensheng <wangwensheng4@huawei.com>
Reviewed-by: Kefeng Wang<wangkefeng.wang@huawei.com>
Reviewed-by: Weilong Chen <chenweilong@huawei.com>
Signed-off-by: Zheng Zengkai <zhengzengkai@huawei.com>
Signed-off-by: Guoqing Jiang <guoqing.jiang@suse.com>
---
 mm/shmem.c | 23 ++++++++++++-----------
 1 file changed, 12 insertions(+), 11 deletions(-)

--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -1579,7 +1579,7 @@ static gfp_t limit_gfp_mask(gfp_t huge_g
 }
 
 static struct page *shmem_alloc_hugepage(gfp_t gfp,
-		struct shmem_inode_info *info, pgoff_t index)
+		struct shmem_inode_info *info, pgoff_t index, int node_id)
 {
 	struct vm_area_struct pvma;
 	struct address_space *mapping = info->vfs_inode.i_mapping;
@@ -1592,7 +1592,7 @@ static struct page *shmem_alloc_hugepage
 		return NULL;
 
 	shmem_pseudo_vma_init(&pvma, info, hindex);
-	page = alloc_pages_vma(gfp, HPAGE_PMD_ORDER, &pvma, 0, numa_node_id(),
+	page = alloc_pages_vma(gfp, HPAGE_PMD_ORDER, &pvma, 0, node_id,
 			       true);
 	shmem_pseudo_vma_destroy(&pvma);
 	if (page)
@@ -1603,13 +1603,14 @@ static struct page *shmem_alloc_hugepage
 }
 
 static struct page *shmem_alloc_page(gfp_t gfp,
-			struct shmem_inode_info *info, pgoff_t index)
+			struct shmem_inode_info *info, pgoff_t index,
+			int node_id)
 {
 	struct vm_area_struct pvma;
 	struct page *page;
 
 	shmem_pseudo_vma_init(&pvma, info, index);
-	page = alloc_page_vma(gfp, &pvma, 0);
+	page = alloc_pages_vma(gfp, 0, &pvma, 0, node_id, false);
 	shmem_pseudo_vma_destroy(&pvma);
 
 	return page;
@@ -1617,7 +1618,7 @@ static struct page *shmem_alloc_page(gfp
 
 static struct page *shmem_alloc_and_acct_page(gfp_t gfp,
 		struct inode *inode,
-		pgoff_t index, bool huge)
+		pgoff_t index, bool huge, int node_id)
 {
 	struct shmem_inode_info *info = SHMEM_I(inode);
 	struct page *page;
@@ -1632,9 +1633,9 @@ static struct page *shmem_alloc_and_acct
 		goto failed;
 
 	if (huge)
-		page = shmem_alloc_hugepage(gfp, info, index);
+		page = shmem_alloc_hugepage(gfp, info, index, node_id);
 	else
-		page = shmem_alloc_page(gfp, info, index);
+		page = shmem_alloc_page(gfp, info, index, node_id);
 	if (page) {
 		__SetPageLocked(page);
 		__SetPageSwapBacked(page);
@@ -1683,7 +1684,7 @@ static int shmem_replace_page(struct pag
 	 * limit chance of success by further cpuset and node constraints.
 	 */
 	gfp &= ~GFP_CONSTRAINT_MASK;
-	newpage = shmem_alloc_page(gfp, info, index);
+	newpage = shmem_alloc_page(gfp, info, index, numa_node_id());
 	if (!newpage)
 		return -ENOMEM;
 
@@ -1916,11 +1917,11 @@ repeat:
 
 	huge_gfp = vma_thp_gfp_mask(vma);
 	huge_gfp = limit_gfp_mask(huge_gfp, gfp);
-	page = shmem_alloc_and_acct_page(huge_gfp, inode, index, true);
+	page = shmem_alloc_and_acct_page(huge_gfp, inode, index, true, numa_node_id());
 	if (IS_ERR(page)) {
 alloc_nohuge:
 		page = shmem_alloc_and_acct_page(gfp, inode,
-						 index, false);
+						 index, false, numa_node_id());
 	}
 	if (IS_ERR(page)) {
 		int retry = 5;
@@ -2393,7 +2394,7 @@ static int shmem_mfill_atomic_pte(struct
 	}
 
 	if (!*pagep) {
-		page = shmem_alloc_page(gfp, info, pgoff);
+		page = shmem_alloc_page(gfp, info, pgoff, numa_node_id());
 		if (!page)
 			goto out_unacct_blocks;
 
