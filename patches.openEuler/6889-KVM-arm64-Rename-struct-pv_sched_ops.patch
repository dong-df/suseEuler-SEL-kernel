From 5a868a49dd40e8c5c7c02f930b0eb725ce75f6c3 Mon Sep 17 00:00:00 2001
From: Wei Li <liwei391@huawei.com>
Date: Tue, 6 Jul 2021 16:50:23 +0800
Subject: [PATCH] KVM: arm64: Rename 'struct pv_sched_ops'
Patch-mainline: Not yet, from openEuler
References: bsn#22
openEuler-commit: 5a868a49dd40e8c5c7c02f930b0eb725ce75f6c3


hulk inclusion
category: feature
bugzilla: 169576
CVE: NA

-------------------------------------------------

Refer to x86, rename 'struct pv_sched_ops sched' to
'struct pv_lock_ops lock' to prepare for supporting CNA on arm64.

Signed-off-by: Wei Li <liwei391@huawei.com>
Reviewed-by: Xie XiuQi <xiexiuqi@huawei.com>
Signed-off-by: Zheng Zengkai <zhengzengkai@huawei.com>
Signed-off-by: Guoqing Jiang <guoqing.jiang@suse.com>
---
 arch/arm64/include/asm/paravirt.h | 14 +++++++-------
 arch/arm64/kernel/paravirt.c      | 16 ++++++++--------
 2 files changed, 15 insertions(+), 15 deletions(-)

diff --git a/arch/arm64/include/asm/paravirt.h b/arch/arm64/include/asm/paravirt.h
index 0e20346bbb75..8630414411be 100644
--- a/arch/arm64/include/asm/paravirt.h
+++ b/arch/arm64/include/asm/paravirt.h
@@ -11,7 +11,7 @@ struct pv_time_ops {
 	unsigned long long (*steal_clock)(int cpu);
 };
 
-struct pv_sched_ops {
+struct pv_lock_ops {
 	void (*queued_spin_lock_slowpath)(struct qspinlock *lock, u32 val);
 	void (*queued_spin_unlock)(struct qspinlock *lock);
 
@@ -23,7 +23,7 @@ struct pv_sched_ops {
 
 struct paravirt_patch_template {
 	struct pv_time_ops time;
-	struct pv_sched_ops sched;
+	struct pv_lock_ops lock;
 };
 
 extern struct paravirt_patch_template pv_ops;
@@ -40,7 +40,7 @@ int __init pv_sched_init(void);
 __visible bool __native_vcpu_is_preempted(int cpu);
 static inline bool pv_vcpu_is_preempted(int cpu)
 {
-	return pv_ops.sched.vcpu_is_preempted(cpu);
+	return pv_ops.lock.vcpu_is_preempted(cpu);
 }
 
 #if defined(CONFIG_SMP) && defined(CONFIG_PARAVIRT_SPINLOCKS)
@@ -48,22 +48,22 @@ void __init pv_qspinlock_init(void);
 bool pv_is_native_spin_unlock(void);
 static inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 {
-	return pv_ops.sched.queued_spin_lock_slowpath(lock, val);
+	return pv_ops.lock.queued_spin_lock_slowpath(lock, val);
 }
 
 static inline void pv_queued_spin_unlock(struct qspinlock *lock)
 {
-	return pv_ops.sched.queued_spin_unlock(lock);
+	return pv_ops.lock.queued_spin_unlock(lock);
 }
 
 static inline void pv_wait(u8 *ptr, u8 val)
 {
-	return pv_ops.sched.wait(ptr, val);
+	return pv_ops.lock.wait(ptr, val);
 }
 
 static inline void pv_kick(int cpu)
 {
-	return pv_ops.sched.kick(cpu);
+	return pv_ops.lock.kick(cpu);
 }
 #else
 
diff --git a/arch/arm64/kernel/paravirt.c b/arch/arm64/kernel/paravirt.c
index 53c539a6b932..847b3c8b1218 100644
--- a/arch/arm64/kernel/paravirt.c
+++ b/arch/arm64/kernel/paravirt.c
@@ -33,10 +33,10 @@ struct static_key paravirt_steal_rq_enabled;
 
 struct paravirt_patch_template pv_ops = {
 #ifdef CONFIG_PARAVIRT_SPINLOCKS
-	.sched.queued_spin_lock_slowpath	= native_queued_spin_lock_slowpath,
-	.sched.queued_spin_unlock		= native_queued_spin_unlock,
+	.lock.queued_spin_lock_slowpath	= native_queued_spin_lock_slowpath,
+	.lock.queued_spin_unlock		= native_queued_spin_unlock,
 #endif
-	.sched.vcpu_is_preempted		= __native_vcpu_is_preempted,
+	.lock.vcpu_is_preempted		= __native_vcpu_is_preempted,
 };
 EXPORT_SYMBOL_GPL(pv_ops);
 
@@ -301,10 +301,10 @@ void __init pv_qspinlock_init(void)
 	pr_info("PV qspinlocks enabled\n");
 
 	__pv_init_lock_hash();
-	pv_ops.sched.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
-	pv_ops.sched.queued_spin_unlock = __pv_queued_spin_unlock;
-	pv_ops.sched.wait = kvm_wait;
-	pv_ops.sched.kick = kvm_kick_cpu;
+	pv_ops.lock.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
+	pv_ops.lock.queued_spin_unlock = __pv_queued_spin_unlock;
+	pv_ops.lock.wait = kvm_wait;
+	pv_ops.lock.kick = kvm_kick_cpu;
 }
 
 static __init int arm_parse_pvspin(char *arg)
@@ -331,7 +331,7 @@ int __init pv_sched_init(void)
 	if (ret)
 		return ret;
 
-	pv_ops.sched.vcpu_is_preempted = kvm_vcpu_is_preempted;
+	pv_ops.lock.vcpu_is_preempted = kvm_vcpu_is_preempted;
 	pr_info("using PV sched preempted\n");
 
 	pv_qspinlock_init();
-- 
2.26.2

