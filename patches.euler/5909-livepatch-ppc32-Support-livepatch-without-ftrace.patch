From e22fb775aaf6027b65682a8d2ab11e242a5f44a0 Mon Sep 17 00:00:00 2001
From: Li Bin <huawei.libin@huawei.com>
Date: Sat, 29 May 2021 14:42:57 +0800
Subject: [PATCH] livepatch/ppc32: Support livepatch without ftrace
Patch-mainline: Not yet, from openEuler
References: bsn#22
openEuler-commit: e22fb775aaf6027b65682a8d2ab11e242a5f44a0


euler inclusion
category: feature
bugzilla: 51924
CVE: NA

----------------------------------------

support livepatch without ftrace for powerpc

supported now:
	livepatch relocation when init_patch after load_module;
	instruction patched when enable;
	activeness function check;
	enforcing the patch stacking principle;

unsupport now:(will fix it feature)
	long jump (both livepatch relocation and insn patched)
	module plts request by livepatch-relocation

Signed-off-by: Cheng Jian <cj.chengjian@huawei.com>
Signed-off-by: Li Bin <huawei.libin@huawei.com>
Tested-by: Wang Feng <wangfeng59@huawei.com>
Reviewed-by: Xie XiuQi <xiexiuqi@huawei.com>
Signed-off-by: zhangyi (F) <yi.zhang@huawei.com>

Signed-off-by: Dong Kai <dongkai11@huawei.com>

Signed-off-by: Ye Weihua <yeweihua4@huawei.com>
Reviewed-by: Kuohai Xu <xukuohai@huawei.com>
Reviewed-by: Yang Jihong <yangjihong1@huawei.com>
Signed-off-by: Zheng Zengkai <zhengzengkai@huawei.com>
Signed-off-by: Guoqing Jiang <guoqing.jiang@suse.com>
---
 arch/powerpc/Kconfig                 |   1 +
 arch/powerpc/include/asm/livepatch.h |  16 ++
 arch/powerpc/kernel/Makefile         |   1 +
 arch/powerpc/kernel/livepatch_32.c   | 250 +++++++++++++++++++++++++++
 4 files changed, 268 insertions(+)
 create mode 100644 arch/powerpc/kernel/livepatch_32.c

diff --git a/arch/powerpc/Kconfig b/arch/powerpc/Kconfig
index 795a62818040..a7d8d84043e6 100644
--- a/arch/powerpc/Kconfig
+++ b/arch/powerpc/Kconfig
@@ -220,6 +220,7 @@ config PPC
 	select HAVE_KRETPROBES
 	select HAVE_LD_DEAD_CODE_DATA_ELIMINATION
 	select HAVE_LIVEPATCH_FTRACE		if HAVE_DYNAMIC_FTRACE_WITH_REGS
+	select HAVE_LIVEPATCH_WO_FTRACE		if PPC32
 	select HAVE_MOD_ARCH_SPECIFIC
 	select HAVE_NMI				if PERF_EVENTS || (PPC64 && PPC_BOOK3S)
 	select HAVE_HARDLOCKUP_DETECTOR_ARCH	if PPC64 && PPC_BOOK3S && SMP
diff --git a/arch/powerpc/include/asm/livepatch.h b/arch/powerpc/include/asm/livepatch.h
index 4a3d5d25fed5..85eed16fad20 100644
--- a/arch/powerpc/include/asm/livepatch.h
+++ b/arch/powerpc/include/asm/livepatch.h
@@ -12,6 +12,7 @@
 #include <linux/sched/task_stack.h>
 
 #ifdef CONFIG_LIVEPATCH
+#ifdef CONFIG_LIVEPATCH_FTRACE
 static inline void klp_arch_set_pc(struct pt_regs *regs, unsigned long ip)
 {
 	regs->nip = ip;
@@ -27,6 +28,21 @@ static inline unsigned long klp_get_ftrace_location(unsigned long faddr)
 	return ftrace_location_range(faddr, faddr + 16);
 }
 
+#elif defined(CONFIG_LIVEPATCH_WO_FTRACE)
+struct klp_func;
+
+/* kernel livepatch instruction barrier */
+#define klp_smp_isb()  __smp_lwsync()
+
+int arch_klp_patch_func(struct klp_func *func);
+void arch_klp_unpatch_func(struct klp_func *func);
+#endif /* CONFIG_LIVEPATCH_FTRACE */
+
+#ifdef CONFIG_LIVEPATCH_STOP_MACHINE_CONSISTENCY
+struct klp_patch;
+int klp_check_calltrace(struct klp_patch *patch, int enable);
+#endif
+
 static inline void klp_init_thread_info(struct task_struct *p)
 {
 	/* + 1 to account for STACK_END_MAGIC */
diff --git a/arch/powerpc/kernel/Makefile b/arch/powerpc/kernel/Makefile
index fe2ef598e2ea..cbe9dbfaf005 100644
--- a/arch/powerpc/kernel/Makefile
+++ b/arch/powerpc/kernel/Makefile
@@ -94,6 +94,7 @@ obj-$(CONFIG_44x)		+= cpu_setup_44x.o
 obj-$(CONFIG_PPC_FSL_BOOK3E)	+= cpu_setup_fsl_booke.o
 obj-$(CONFIG_PPC_DOORBELL)	+= dbell.o
 obj-$(CONFIG_JUMP_LABEL)	+= jump_label.o
+obj-$(CONFIG_LIVEPATCH_WO_FTRACE) += livepatch_$(BITS).o
 
 extra-$(CONFIG_PPC64)		:= head_64.o
 extra-$(CONFIG_PPC_BOOK3S_32)	:= head_book3s_32.o
diff --git a/arch/powerpc/kernel/livepatch_32.c b/arch/powerpc/kernel/livepatch_32.c
new file mode 100644
index 000000000000..9a6585ca0fa3
--- /dev/null
+++ b/arch/powerpc/kernel/livepatch_32.c
@@ -0,0 +1,250 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * livepatch.c - powerpc-specific Kernel Live Patching Core
+ *
+ * Copyright (C) 2018  Huawei Technologies Co., Ltd.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2
+ * of the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, see <http://www.gnu.org/licenses/>.
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/module.h>
+#include <linux/uaccess.h>
+#include <linux/livepatch.h>
+#include <linux/sched/debug.h>
+#include <asm/livepatch.h>
+#include <asm/cacheflush.h>
+#include <linux/slab.h>
+#include <asm/code-patching.h>
+
+#if defined (CONFIG_LIVEPATCH_STOP_MACHINE_CONSISTENCY) || \
+    defined (CONFIG_LIVEPATCH_WO_FTRACE)
+#define INSN_SIZE	4
+
+struct klp_func_node {
+	struct list_head node;
+	struct list_head func_stack;
+	void *old_func;
+	u32	old_insn;
+};
+
+static LIST_HEAD(klp_func_list);
+
+static struct klp_func_node *klp_find_func_node(void *old_func)
+{
+	struct klp_func_node *func_node;
+
+	list_for_each_entry(func_node, &klp_func_list, node) {
+		if (func_node->old_func == old_func)
+			return func_node;
+	}
+
+	return NULL;
+}
+#endif
+
+#ifdef CONFIG_LIVEPATCH_STOP_MACHINE_CONSISTENCY
+struct stackframe {
+	unsigned long sp;
+	unsigned long pc;
+};
+
+struct walk_stackframe_args {
+	struct klp_patch *patch;
+	int enable;
+	int ret;
+};
+
+static inline int klp_compare_address(unsigned long pc, unsigned long func_addr,
+				unsigned long func_size, const char *func_name)
+{
+	if (pc >= func_addr && pc < func_addr + func_size) {
+		pr_err("func %s is in use!\n", func_name);
+		return -EBUSY;
+	}
+	return 0;
+}
+
+static int klp_check_activeness_func(struct stackframe *frame, void *data)
+{
+	struct walk_stackframe_args *args = data;
+	struct klp_patch *patch = args->patch;
+	struct klp_object *obj;
+	struct klp_func *func;
+	unsigned long func_addr, func_size;
+	const char *func_name;
+
+	if (args->ret)
+		return args->ret;
+
+	for (obj = patch->objs; obj->funcs; obj++) {
+		for (func = obj->funcs; func->old_name; func++) {
+			if (args->enable) {
+				func_addr = (unsigned long)func->old_func;
+				func_size = func->old_size;
+			} else {
+				func_addr = (unsigned long)func->new_func;
+				func_size = func->new_size;
+			}
+			func_name = func->old_name;
+			args->ret = klp_compare_address(frame->pc, func_addr,
+					func_size, func_name);
+			if (args->ret)
+				return args->ret;
+		}
+	}
+
+	return args->ret;
+}
+
+static int unwind_frame(struct task_struct *tsk, struct stackframe *frame)
+{
+
+	unsigned long *stack;
+
+	if (!validate_sp(frame->sp, tsk, STACK_FRAME_OVERHEAD))
+		return -1;
+
+	stack = (unsigned long *)frame->sp;
+	frame->sp = stack[0];
+	frame->pc = stack[STACK_FRAME_LR_SAVE];
+	return 0;
+}
+
+void notrace klp_walk_stackframe(struct stackframe *frame,
+		int (*fn)(struct stackframe *, void *),
+		struct task_struct *tsk, void *data)
+{
+	while (1) {
+		int ret;
+
+		if (fn(frame, data))
+			break;
+		ret = unwind_frame(tsk, frame);
+		if (ret < 0)
+			break;
+	}
+}
+
+int klp_check_calltrace(struct klp_patch *patch, int enable)
+{
+	struct task_struct *g, *t;
+	struct stackframe frame;
+	unsigned long *stack;
+	int ret = 0;
+
+	struct walk_stackframe_args args = {
+		.patch = patch,
+		.enable = enable,
+		.ret = 0
+	};
+
+	for_each_process_thread(g, t) {
+		stack = (unsigned long *)t->thread.ksp;
+		frame.sp = (unsigned long)stack;
+		frame.pc = stack[STACK_FRAME_LR_SAVE];
+		klp_walk_stackframe(&frame, klp_check_activeness_func,
+				t, &args);
+		if (args.ret) {
+			ret = args.ret;
+			pr_info("PID: %d Comm: %.20s\n", t->pid, t->comm);
+			show_stack(t, NULL, KERN_INFO);
+			goto out;
+		}
+	}
+
+out:
+	return ret;
+}
+#endif
+
+#ifdef CONFIG_LIVEPATCH_WO_FTRACE
+int arch_klp_patch_func(struct klp_func *func)
+{
+	struct klp_func_node *func_node;
+	unsigned long pc, new_addr;
+	struct ppc_inst instr;
+	long ret;
+	int memory_flag = 0;
+
+	func_node = klp_find_func_node(func->old_func);
+	if (!func_node) {
+		func_node = kzalloc(sizeof(*func_node), GFP_ATOMIC);
+		if (!func_node)
+			return -ENOMEM;
+
+		memory_flag = 1;
+		INIT_LIST_HEAD(&func_node->func_stack);
+		func_node->old_func = func->old_func;
+		ret = copy_from_kernel_nofault(&func_node->old_insn,
+					       func->old_func, INSN_SIZE);
+		if (ret) {
+			kfree(func_node);
+			return -EPERM;
+		}
+		list_add_rcu(&func_node->node, &klp_func_list);
+	}
+
+	list_add_rcu(&func->stack_node, &func_node->func_stack);
+
+	pc = (unsigned long)func->old_func;
+	new_addr = (unsigned long)func->new_func;
+	create_branch(&instr, (struct ppc_inst *)pc, new_addr, 0);
+
+	ret = patch_instruction((struct ppc_inst *)pc, instr);
+	if (ret)
+		goto ERR_OUT;
+
+	return 0;
+
+ERR_OUT:
+	list_del_rcu(&func->stack_node);
+	if (memory_flag) {
+		list_del_rcu(&func_node->node);
+		kfree(func_node);
+	}
+
+	return -EPERM;
+}
+
+void arch_klp_unpatch_func(struct klp_func *func)
+{
+	struct klp_func_node *func_node;
+	struct klp_func *next_func;
+	unsigned long pc, new_addr;
+	struct ppc_inst instr;
+	u32 insn;
+
+	func_node = klp_find_func_node(func->old_func);
+	pc = (unsigned long)func_node->old_func;
+	if (list_is_singular(&func_node->func_stack)) {
+		insn = func_node->old_insn;
+		list_del_rcu(&func->stack_node);
+		list_del_rcu(&func_node->node);
+		kfree(func_node);
+
+		patch_instruction((struct ppc_inst *)pc, ppc_inst(insn));
+	} else {
+		list_del_rcu(&func->stack_node);
+		next_func = list_first_or_null_rcu(&func_node->func_stack,
+					struct klp_func, stack_node);
+
+		new_addr = (unsigned long)next_func->new_func;
+		create_branch(&instr, (struct ppc_inst *)pc, new_addr, 0);
+
+		patch_instruction((struct ppc_inst *)pc, instr);
+	}
+}
+#endif
-- 
2.26.2

