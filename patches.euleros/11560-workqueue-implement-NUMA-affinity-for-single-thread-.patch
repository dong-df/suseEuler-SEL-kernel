From 1ffce0db44534281918b5833ba200a4368ec3ae0 Mon Sep 17 00:00:00 2001
From: Biaoxiang Ye <yebiaoxiang@huawei.com>
Date: Fri, 26 Nov 2021 16:26:51 +0800
Subject: [PATCH] workqueue: implement NUMA affinity for single thread
 workqueue
Patch-mainline: Not yet, from openEuler
References: bsn#22
openEuler-commit: 1ffce0db44534281918b5833ba200a4368ec3ae0


euleros inclusion
category: feature
bugzilla: https://gitee.com/openeuler/kernel/issues/I4IZNO
CVE: NA

-------------------------------------------------

Currently, single thread workqueue only have single pwq, all of
works are queued the same workerpool. This is not optimal on
NUMA machines, will cause workers jump around across node.

This patch add a new wq flags __WQ_DYNAMIC,  this new kind of
single thread workqueue creates a separate pwq covering the
intersecting CPUS for each NUMA node which has online CPUS
in @attrs->cpumask instead of mapping all entries of numa_pwq_tbl[]
to the same pwq. After this, we can specify the @cpu of
queue_work_on, so the work can be executed on the same NUMA
node of the specified @cpu.
This kind of wq only support single work, multi works can't guarantee
the work's order.

Signed-off-by: Biaoxiang Ye <yebiaoxiang@huawei.com>
Acked-by: Hanjun Guo <guohanjun@huawei.com>
Signed-off-by: Yang Yingliang <yangyingliang@huawei.com>
Reviewed-by: fang yi <eric.fangyi@huawei.com>
Signed-off-by: Zheng Zengkai <zhengzengkai@huawei.com>
Signed-off-by: Guoqing Jiang <guoqing.jiang@suse.com>
---
 kernel/workqueue.c | 15 ++++++++++-----
 1 file changed, 10 insertions(+), 5 deletions(-)

diff --git a/kernel/workqueue.c b/kernel/workqueue.c
index 4cb622b2661b..29a677697fd4 100644
--- a/kernel/workqueue.c
+++ b/kernel/workqueue.c
@@ -3938,6 +3938,9 @@ apply_wqattrs_prepare(struct workqueue_struct *wq,
 	 * wq_unbound_cpumask, we fallback to the wq_unbound_cpumask.
 	 */
 	copy_workqueue_attrs(new_attrs, attrs);
+	if (wq->flags & __WQ_DYNAMIC)
+		new_attrs->no_numa = false;
+
 	cpumask_and(new_attrs->cpumask, new_attrs->cpumask, wq_unbound_cpumask);
 	if (unlikely(cpumask_empty(new_attrs->cpumask)))
 		cpumask_copy(new_attrs->cpumask, wq_unbound_cpumask);
@@ -4193,10 +4196,12 @@ static int alloc_and_link_pwqs(struct workqueue_struct *wq)
 	get_online_cpus();
 	if (wq->flags & __WQ_ORDERED) {
 		ret = apply_workqueue_attrs(wq, ordered_wq_attrs[highpri]);
-		/* there should only be single pwq for ordering guarantee */
-		WARN(!ret && (wq->pwqs.next != &wq->dfl_pwq->pwqs_node ||
-			      wq->pwqs.prev != &wq->dfl_pwq->pwqs_node),
-		     "ordering guarantee broken for workqueue %s\n", wq->name);
+		if (!(wq->flags & __WQ_DYNAMIC)) {
+			/* there should only be single pwq for ordering guarantee */
+			WARN(!ret && (wq->pwqs.next != &wq->dfl_pwq->pwqs_node ||
+					wq->pwqs.prev != &wq->dfl_pwq->pwqs_node),
+					"ordering guarantee broken for workqueue %s\n", wq->name);
+		}
 	} else {
 		ret = apply_workqueue_attrs(wq, unbound_std_wq_attrs[highpri]);
 	}
@@ -5288,7 +5293,7 @@ static int workqueue_apply_unbound_cpumask(void)
 		if (!(wq->flags & WQ_UNBOUND))
 			continue;
 		/* creating multiple pwqs breaks ordering guarantee */
-		if (wq->flags & __WQ_ORDERED)
+		if ((wq->flags & __WQ_ORDERED) && !(wq->flags & __WQ_DYNAMIC))
 			continue;
 
 		ctx = apply_wqattrs_prepare(wq, wq->unbound_attrs);
-- 
2.26.2

